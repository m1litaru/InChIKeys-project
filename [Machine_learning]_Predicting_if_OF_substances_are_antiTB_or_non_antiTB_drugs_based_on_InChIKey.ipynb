{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Machine learning] Predicting if OF-substances are antiTB or non-antiTB drugs based on InChIKey",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvtvaCm8vTe5"
      },
      "source": [
        "The first block sets up the environment, installing all the necessary libraries and functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_17jwdHF5up"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLhVNAbCtBru"
      },
      "source": [
        "This script transforms InChIKeys from strings into an array with 25 integers. (InChIKey codes have 27 characters, but we omit the dashes (â€“).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYI40E0y__iW"
      },
      "source": [
        "# transform one InChIKey into an int array\n",
        "def convert_InChIKeyToInt(InChIKey):\n",
        "  numeric = ''\n",
        "  for c in InChIKey:\n",
        "    if(c.isalpha()):\n",
        "      numeric += str(ord(c))+','\n",
        "  return numeric[:-1]\n",
        "\n",
        "def convert_InChIKeyToIntArray(InChIKey):\n",
        "  numeric = np.zeros(25)\n",
        "  i = 0\n",
        "  for c in InChIKey:\n",
        "    if(c.isalpha()):\n",
        "      numeric[i] = ord(c)\n",
        "      i += 1\n",
        "  return numeric. astype(int)\n",
        "\n",
        "# transform all InChIKeys from one .csv file into int arrays\n",
        "def getNumericInChIKeysFromFile(filename, noOfDrugs):\n",
        "  dataset = pd.read_csv(filename)\n",
        "  for i in range(0,noOfDrugs):\n",
        "    print(convert_InChIKeyToInt(dataset.iloc[i,1:2].values[0]))\n",
        "\n",
        "# e.g.: getNumericInChIKeysFromFile('non-antiTB InChIKey (strings).csv', 71)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UECY4zPYud3b"
      },
      "source": [
        "We parse the three *.csv* files (one file for antiTB drugs, one for non-antiTB drugs and one for OF- substances) containing the numeric InChIKeys (after the transformation part previously described) and we create one dataset with the drugs from each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1nEeZ3uFa3u"
      },
      "source": [
        "test_dataset = pd.read_csv('test - data.csv')\n",
        "dataset = pd.read_csv('antiTB InChIKey (numeric).csv')\n",
        "dataset_negative = pd.read_csv('non-antiTB InChIKey (numeric).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ-WancF6asu"
      },
      "source": [
        "Because the numeric values obtained after the InChIKey was transformed from *char* to *int* are biased (A is 65 in 'ASCII code', Z is 90), we normalize these values, using a scaller (*StandardScaler*). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vdAEtLhL3QH"
      },
      "source": [
        "#print(dataset.iloc[0,:26].values) # we have 25 inchi letters/numbers and 1 label\n",
        "array = np.arange(25)\n",
        "dataset.iloc[:,array] = StandardScaler().fit_transform(dataset.iloc[:,array].values)\n",
        "dataset_negative.iloc[:,array] = StandardScaler().fit_transform(dataset_negative.iloc[:,array].values)\n",
        "test_dataset.iloc[:,array] = StandardScaler().fit_transform(test_dataset.iloc[:,array].values)\n",
        "x_not_known = test_dataset.iloc[:,:25].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1G3f3Jc3sGy"
      },
      "source": [
        "We split each dataset (dataset, dataset_negative) in 2 datasets: *x* and *y*, where *x* contain the features (*x* is a matrix, the ML model's input, containing the numeric scaled InCHIKeys' values) and *y* is the expected outcome (*y* is an array, containg for each drug an *int* value: *0* - non-antiTB, *1* - antiTB). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiH6t7VifnTd"
      },
      "source": [
        "x_positive = dataset.iloc[:,:25].values\n",
        "y_positive = dataset.iloc[:,25:26].values\n",
        "\n",
        "x_negative = dataset_negative.iloc[:,:25].values\n",
        "y_negative = dataset_negative.iloc[:,25:26].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxKjPtzt4Vrp"
      },
      "source": [
        "Then, each of the previously mentioned array is split into *train* and *test*. We use the *train* arrays to create the model and the *test* arrays to compute the accuracy of that model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FN6Syvx4U54"
      },
      "source": [
        "X_pos_train,X_pos_test,y_pos_train,y_pos_test = train_test_split(x_positive,y_positive,test_size=0.15,shuffle = True,random_state=1)\n",
        "X_neg_train,X_neg_test,y_neg_train,y_neg_test = train_test_split(x_negative[51:75],y_negative[51:75],test_size=0.15,shuffle = True)\n",
        "\n",
        "X_train = np.concatenate((X_pos_train, X_neg_train))\n",
        "X_test = np.concatenate((X_pos_test, X_neg_test))\n",
        "y_train = np.concatenate((y_pos_train, y_neg_train))\n",
        "y_test = np.concatenate((y_pos_test, y_neg_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgf8YT-46FN"
      },
      "source": [
        "The next cells contain the initialization and parametrization of the used classifiers (*KMeans*, *SVC*, *LinearSVC*, *Logistic Regression*, a.s.o.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5b47K6IeyTy"
      },
      "source": [
        "classifier = KMeans(n_clusters=2, init='random', n_init=10000, max_iter=100000, \n",
        "                    tol=0.00001, precompute_distances=True, verbose=0, random_state=1, \n",
        "                    copy_x=True, algorithm='elkan')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mC2PTbNiDAL"
      },
      "source": [
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "classifier = SVC()\n",
        "classifier = LinearSVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L95ZO0ltiPhQ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression(multi_class='ovr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_zDnMAuiWo5"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9FARFK5magv"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "classifier = GradientBoostingClassifier(n_estimators=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Kmd99XnqNV"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "classifier = MLPClassifier(max_iter=500, hidden_layer_sizes = (150,),\n",
        "                           activation= 'relu', solver = 'adam',\n",
        "                           learning_rate= 'invscaling',random_state = 0)\n",
        "classifier = MLPClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_SL0tedoHK3"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WLfBi_-l05t"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier \n",
        "classifier = DecisionTreeClassifier(max_depth=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vis0IcCg5U0I"
      },
      "source": [
        "We create the model and then compute the accuracy for each model for both datasets: training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI9GxjqQYJTt"
      },
      "source": [
        "classifier.fit(X_train, y_train)\n",
        "y_pred_train = classifier.predict(X_train)\n",
        "a_train = accuracy_score(y_true=y_train, y_pred=y_pred_train)\n",
        "\n",
        "y_pred_test = classifier.predict(X_test)\n",
        "a_test = accuracy_score(y_true=y_test, y_pred=y_pred_test)\n",
        "\n",
        "print('Training acc:', a_train)\n",
        "print('Test acc:', a_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qprSkv135y4V"
      },
      "source": [
        "We apply the model and then predict the outcome for the array containing OF-substances' properties, printing the results in the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvAFuXtKhSep",
        "outputId": "c4c1d0c9-6bf6-4939-800a-73ee93073e0c"
      },
      "source": [
        "y_pred = classifier.predict(x_not_known)\n",
        "for i in range(0,22):\n",
        "  print(str(y_pred[i])+','+str(test_dataset.iloc[i,26:27].values[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,OF-1180\n",
            "1,OF-1182\n",
            "1,OF-1187\n",
            "1,OF-1189\n",
            "1,OF-1227\n",
            "1,OF-1242\n",
            "1,OF-1250\n",
            "1,OF-1253\n",
            "1,OF-1273\n",
            "1,OF-1276\n",
            "1,OF-1279\n",
            "1,OF-1283\n",
            "1,OF-1285\n",
            "1,OF-1288\n",
            "1,OF-1289\n",
            "1,OF-1290\n",
            "1,OF-1292\n",
            "1,OF-1294\n",
            "1,OF-1295\n",
            "1,OF-1264\n",
            "1,OF-1272\n",
            "1,OF-242\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}